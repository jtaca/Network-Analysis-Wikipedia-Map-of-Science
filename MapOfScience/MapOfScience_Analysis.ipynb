{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = open('MapOfScience.gml', 'r').read()\n",
    "document = document.split('Version 1.0')[1]\n",
    "document = document.split('node\\n')[1:]\n",
    "nodes = document[:687]\n",
    "nodes.append(document[686:][0].split('edge')[0])\n",
    "final_nodes=[]\n",
    "node_classes = {'Social':0,'Formal':0,'Natural':0,'Applied':0}\n",
    "for node in nodes:\n",
    "    node_id = int(node.split('id')[1][:5].split(' ')[1])\n",
    "    node_name = node.split('name \"')[1].split('\"')[0]\n",
    "    node_class = node.split('Class \"')[1].split('\"')[0]\n",
    "    node_url = node.split('WikipediaUrl \"')[1].split('\"')[0]\n",
    "    final_nodes.append((node_id, {'name':node_name,'class': node_class,'url': node_url }))\n",
    "    node_classes[node_class] = node_classes[node_class] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(final_nodes)\n",
    "    \n",
    "edges = document[686:][0].split('edge')[1:]\n",
    "final_egde = [] \n",
    "\n",
    "for edge in edges:\n",
    "    source = int(edge.split('source')[1][:5].split(' ')[1])\n",
    "    target = int(edge.split('target')[1][:5].split(' ')[1])\n",
    "    CosineSimilarity = float(edge.split('CosineSimilarity')[1][:18].split(' ')[1])\n",
    "    G.add_edge(source, target, weight = CosineSimilarity)\n",
    "\n",
    "for i in node_classes:\n",
    "    node_classes[i] = node_classes[i]/len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 687\n",
      "Number of edges: 6523\n",
      "Average degree:  18.9898\n",
      "\n",
      "Example node: \n",
      "(0, {'name': 'Accounting', 'class': 'Applied', 'url': 'https://en.wikipedia.org/wiki/Accounting'})\n",
      "\n",
      "Percentage of node science classes: \n",
      "{'Applied': 0.13682678311499272,\n",
      " 'Formal': 0.28093158660844253,\n",
      " 'Natural': 0.24890829694323144,\n",
      " 'Social': 0.33478893740902477}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(nx.info(G)) \n",
    "print('\\nExample node: ')\n",
    "print(final_nodes[0])\n",
    "print('\\nPercentage of node science classes: ')\n",
    "pprint(node_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected components outside of the main component:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{327, 484, 569}, {377, 401, 615}, {42, 67}, {500, 598}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can see that \n",
    "print('Connected components outside of the main component:')\n",
    "sorted(nx.connected_components(G), key = len, reverse=True)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydrography as a Social Science\n",
      "Coastal geography as a Social Science\n",
      "Coastal geography as a Natural Science\n",
      "\n",
      "\n",
      "Marine biology as a Natural Science\n",
      "Oceanography as a Natural Science\n",
      "Oceanography as a Social Science\n",
      "\n",
      "\n",
      "Food science as a Applied Science\n",
      "Nutrition as a Applied Science\n",
      "\n",
      "\n",
      "Cuisine as a Social Science\n",
      "Meal as a Social Science\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodeDict = dict(G.nodes(data=True))\n",
    "for i in sorted(nx.connected_components(G), key = len, reverse=True)[1:]:\n",
    "    for j in i:\n",
    "        print(nodeDict[j]['name']+' as a '+nodeDict[j]['class']+' Science')\n",
    "    print('\\n')\n",
    "    \n",
    "#Here we can see the Science pages that were not \n",
    "#connected to the main component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POWER LAW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree_centrality: \n",
      "{'class': 'Social',\n",
      " 'name': 'School psychology',\n",
      " 'url': 'https://en.wikipedia.org/wiki/School_psychology'}\n",
      "eigenvector_centrality: \n",
      "{'class': 'Social',\n",
      " 'name': 'School psychology',\n",
      " 'url': 'https://en.wikipedia.org/wiki/School_psychology'}\n",
      "betweenness_centrality: \n",
      "{'class': 'Natural',\n",
      " 'name': 'Population biology',\n",
      " 'url': 'https://en.wikipedia.org/wiki/Population_biology'}\n"
     ]
    }
   ],
   "source": [
    "#Centrality\n",
    "def get_max_dic(dic):\n",
    "    maxy = float(0.00)\n",
    "    max_id = 0\n",
    "    for i in range(len(dic)):\n",
    "        if dic[i] > maxy:\n",
    "            maxy = dic[i]\n",
    "            max_id = i\n",
    "\n",
    "    nodeDict = dict(G.nodes(data=True))\n",
    "    #print(max_id)\n",
    "    return nodeDict[max_id]\n",
    "print('degree_centrality: ')\n",
    "pprint(get_max_dic(nx.degree_centrality(G)))\n",
    "print('eigenvector_centrality: ')\n",
    "pprint(get_max_dic(nx.eigenvector_centrality(G)))\n",
    "print('betweenness_centrality: ')\n",
    "pprint(get_max_dic(nx.betweenness_centrality(G)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economics as a Social Science\n",
      "Algorithm as a Formal Science\n",
      "Population biology as a Natural Science\n"
     ]
    }
   ],
   "source": [
    "bet_dict = nx.betweenness_centrality(G)\n",
    "sorted_bet_dict = [k for k, v in sorted(bet_dict.items(), key=lambda item: item[1])]\n",
    "for i in sorted_bet_dict[-3:]:\n",
    "       print(nodeDict[i]['name']+' as a '+nodeDict[i]['class']+' Science')\n",
    "#Here we see the 3 pages with highest betweenness centrality ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CLUSTER COEFFICIENT \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
